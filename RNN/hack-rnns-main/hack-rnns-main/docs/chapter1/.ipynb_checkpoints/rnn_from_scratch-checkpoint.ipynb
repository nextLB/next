{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbc26a7-be2e-45d6-96c0-e50edd84e5f0",
   "metadata": {},
   "source": [
    "# RNN 简介\n",
    "\n",
    "# 时间序列模型概念简介\n",
    "简介时序模型的不同之处\n",
    "\n",
    "# RNN网络结构图\n",
    "![rnn](images/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed32eb-d5dc-4fe6-9164-3d621a98d2fa",
   "metadata": {},
   "source": [
    "RNN公式：\n",
    "![rnn_rule](images/rnn_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51511bf4-3182-4f3d-8482-5f36d3ab755b",
   "metadata": {},
   "source": [
    "# 观察torch.nn.RNN的输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e152dd0d-8abf-46d3-85d4-f418335ad2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98eb8147-8728-49ac-b7ec-bbd4c85e27bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1926, -0.5641, -0.1246],\n",
      "         [ 0.3857, -0.5942,  0.3756],\n",
      "         [-0.7565, -0.9860, -0.6089],\n",
      "         [ 0.1879, -0.8991, -0.3685],\n",
      "         [ 0.4113, -0.8877, -0.5903]]], grad_fn=<TransposeBackward1>)\n",
      "torch.Size([1, 5, 3])\n",
      "tensor([[[ 0.4113, -0.8877, -0.5903]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 单向、单层rnn\n",
    "single_rnn = nn.RNN(input_size=4, hidden_size=3, num_layers=1, batch_first=True) # batch_first=True表示输入数据的维度为[batch_size, seq_len, input_size]\n",
    "input = torch.randn(1, 5, 4) # 输入数据维度为[batch_size, seq_len, input_size]\n",
    "output, h_n = single_rnn(input) # output维度为[batch_size, seq_len, hidden_size=3]，h_n维度为[num_layers=1, batch_size, hidden_size=3]\n",
    "print(output, output.shape, h_n, h_n.shape,  sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc7e1b-e015-4be3-8047-567c8c42f78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7565, -0.9860, -0.6089]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:, 2, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c81d55-80f1-496e-8c00-57db4bf6edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9352, -0.1535,  0.3298,  0.8335, -0.9024, -0.6281],\n",
      "         [ 0.3034, -0.5223, -0.9183,  0.8817, -0.4630, -0.6553],\n",
      "         [ 0.9745, -0.4444,  0.7889,  0.9376, -0.6616, -0.8148],\n",
      "         [ 0.7716, -0.2623, -0.8482,  0.7856, -0.1788, -0.9494],\n",
      "         [ 0.7237, -0.5549, -0.1000,  0.7960, -0.4034, -0.1305]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "torch.Size([1, 5, 6])\n",
      "tensor([[[ 0.7237, -0.5549, -0.1000]],\n",
      "\n",
      "        [[ 0.8335, -0.9024, -0.6281]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 双向、单层rnn\n",
    "bi_rnn = nn.RNN(input_size=4, hidden_size=3, num_layers=1, batch_first=True, bidirectional=True)\n",
    "bi_output, bi_h_n = bi_rnn(input)\n",
    "print(bi_output, bi_output.shape, bi_h_n, bi_h_n.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0b04f-d59f-421a-b5b8-6d8f87b8d06e",
   "metadata": {},
   "source": [
    "# 从零手搓 RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444273e-68ea-446f-9332-c7c6c45a6a52",
   "metadata": {},
   "source": [
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fa97ad3-ab64-4bd9-9b35-79601ec6960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c36426-ec56-479a-aa22-6ec69404afa8",
   "metadata": {},
   "source": [
    "## 定义 RNN 模型类\n",
    "创建一个继承自nn.Module的类来定义我们的 RNN 模型结构，在类中要实现__init__构造函数用于初始化模型的各层，以及forward方法用于定义前向传播过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0b3ab26-3e6a-47c8-82c6-481af6f25580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539a452-143e-423c-bfca-6cd925a40f08",
   "metadata": {},
   "source": [
    "在上述代码中：\n",
    "\n",
    "- `__init__`方法：\n",
    "接受参数input_size（输入特征的维度，比如对于文本的词向量表示，就是词向量的维度）、hidden_size（RNN 隐藏层的神经元数量）、num_layers（RNN 的层数）、num_classes（要预测的类别数量）。\n",
    "创建了nn.RNN实例，它代表了基本的循环神经网络层，batch_first=True表示输入张量的第一个维度是批次大小，符合常见的数据组织形式便于使用。\n",
    "同时定义了一个全连接层self.fc，用于将 RNN 层输出的特征转换为对应类别数量的得分。\n",
    "\n",
    "- `forward`方法：\n",
    "首先初始化了隐藏状态h0，其维度根据 RNN 的层数、批次大小和隐藏层大小来确定，并确保其与输入数据x在同一个设备（CPU 或 GPU）上。\n",
    "然后将输入x和初始隐藏状态h0传入rnn层进行前向传播，得到输出out和最终的隐藏状态（这里用_忽略最终隐藏状态，因为我们主要关注最后时刻的输出用于分类）。\n",
    "接着取out中最后一个时间步的输出，通过全连接层self.fc将其转换为类别预测的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcc624-bed9-44c1-8fbe-7e98a46dc37d",
   "metadata": {},
   "source": [
    "## 使用模型示例\n",
    "以下是简单的示例代码展示如何实例化模型、创建输入数据并进行前向传播得到预测结果（这里假设输入数据维度合适且已经经过必要的预处理，比如文本数据已经转换为词向量序列等情况）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13db92ca-487c-489f-94de-760a08778c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# 假设输入特征维度为10，隐藏层大小为20，2层RNN，要预测5个类别\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "seq_length = 10\n",
    "\n",
    "# 创建模型实例\n",
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# 创建随机输入数据，维度为 (batch_size, seq_length, input_size)\n",
    "input_data = torch.rand(batch_size, seq_length, input_size)\n",
    "\n",
    "# 前向传播得到输出（预测结果）\n",
    "output = model(input_data)\n",
    "print(output.shape)  # 输出形状应该为 (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a24104-50d4-453d-873b-6eb67129dbd4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c41de90-b948-4c9c-ac70-11300ac4da29",
   "metadata": {},
   "source": [
    "上述代码：\n",
    "- 首先定义了模型相关的参数，如输入特征维度、隐藏层大小、层数、类别数量以及输入数据的批次大小和序列长度等。\n",
    "- 接着实例化了RNNModel类创建了模型对象model。\n",
    "- 然后生成了随机的符合要求维度的输入数据input_data，并将其传入模型进行前向传播得到输出output，最后打印输出的形状，确认其符合预期（批次大小对应的每个样本都有对应num_classes个预测得分）。\n",
    "  \n",
    "请注意，这只是一个非常基础的 RNN 模型实现示例，在实际应用中，可能还需要根据具体任务来调整模型结构、添加更多的功能，比如：\n",
    "- 可以使用其他的 RNN 变种，如LSTM（长短期记忆网络）、GRU（门控循环单元），只需要将nn.RNN替换为nn.LSTM或者nn.GRU等，相应的初始化参数和返回值的处理会稍有不同。\n",
    "- 对输入数据通常需要更严谨的预处理，比如对于文本数据要进行合适的分词、构建词表、将文本转为词向量等操作，对于数值型序列数据可能要进行归一化等处理。\n",
    "- 还需要定义合适的损失函数、优化器等来进行模型的训练以及评估模型的性能等，这些都是完整构建一个可用的基于 RNN 的深度学习模型的重要环节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717b005-b7ad-4d91-a498-8e198823251a",
   "metadata": {},
   "source": [
    "## 训练和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b5b46-2e55-43a9-8f4c-ab271cfdbb66",
   "metadata": {},
   "source": [
    "定义损失函数和优化器：\n",
    "损失函数：根据任务类型选择合适的损失函数，如对于分类任务常用交叉熵损失函数nn.CrossEntropyLoss()，对于回归任务常用均方误差损失函数nn.MSELoss()等。\n",
    "优化器：选择合适的优化器来更新模型的参数，如随机梯度下降（SGD）、Adagrad、Adadelta、RMSProp、Adam 等优化器。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa7ff74c-6c07-4adf-9ee8-8e5caf9b5916",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m RNNModel(input_size, hidden_size, num_layers, num_classes)\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[43mlearning_rate\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7733adc-c9e0-4d56-bfdd-355beda0dc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fabf7-b89e-4e12-bf75-e72e5f335922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ae89c-d8aa-451a-9906-ced8b82770b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "训练模型：\n",
    "在训练集上对模型进行多次迭代训练，每次迭代称为一个 epoch。在每个 epoch 中，将训练数据分成小批次（batch），对每个批次进行以下操作：\n",
    "前向传播：将输入数据传入模型，得到模型的输出。\n",
    "计算损失：根据模型的输出和真实标签，计算损失值。\n",
    "反向传播：调用loss.backward()方法，计算损失对模型参数的梯度。\n",
    "更新参数：使用优化器的step()方法，根据计算得到的梯度更新模型的参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943a164-b997-4947-bce8-8618b421ef3d",
   "metadata": {},
   "source": [
    "## forword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52299c8-f30e-4f86-8f39-c1a35116dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546887fc-623d-4623-b271-a77b6bf0abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, input_size, hidden_size = 2, 3, 2, 3 # 批次大小、序列长度、输入维度、隐藏层维度\n",
    "num_layers = 1 # rnn层数\n",
    "\n",
    "input = torch.randn(batch_size, seq_len, input_size) # 初始化输入数据\n",
    "h_prev = torch.zeros(batch_size, hidden_size) # 初始化隐藏层状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4beb290-ebac-4347-9a66-40b7360d178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8950,  0.4833, -0.5547],\n",
      "         [ 0.1424,  0.4373, -0.6090],\n",
      "         [-0.5399,  0.7522, -0.0882]],\n",
      "\n",
      "        [[ 0.2022,  0.8119, -0.3363],\n",
      "         [ 0.0075,  0.5236, -0.5589],\n",
      "         [ 0.6017,  0.2810, -0.6089]]], grad_fn=<TransposeBackward1>)\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[-0.5399,  0.7522, -0.0882],\n",
      "         [ 0.6017,  0.2810, -0.6089]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # 初始化rnn\n",
    "rnn_output, h_n = rnn(input, h_prev.unsqueeze(0)) # rnn输出和隐藏层状态\n",
    "print(rnn_output, rnn_output.shape, h_n, h_n.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc017fe9-187a-4981-8f06-1a84680574cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight_ih_l0',\n",
       "              tensor([[ 0.5479,  0.4916],\n",
       "                      [-0.2934, -0.2234],\n",
       "                      [-0.2745, -0.0150]])),\n",
       "             ('weight_hh_l0',\n",
       "              tensor([[ 0.1761, -0.3001,  0.5395],\n",
       "                      [-0.2634, -0.2903,  0.3202],\n",
       "                      [-0.4855,  0.2617, -0.0028]])),\n",
       "             ('bias_ih_l0', tensor([ 0.4256,  0.4981, -0.3173])),\n",
       "             ('bias_hh_l0', tensor([ 0.1950,  0.4163, -0.2147]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad63aab-300f-45c0-9d46-bb3e9c58d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN：\n",
    "def __init__(self,input_size=4, hidden_size=3, num_layers=1, batch_first=True):\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "def rnn_forward(input, W_ih, W_hh, b_ih, b_hh, h_prev):\n",
    "    batch_size, seq_len, input_size = input.shape\n",
    "    hidden_size = W_ih.shape[0] # 隐藏层维度, seq_len就等于hidden_size，所以是W_ih.shape[0]\n",
    "    h_output = torch.zeros(batch_size, seq_len, hidden_size) # 初始化一个输出矩阵output 看官方参数来定义\n",
    "    for t in range(seq_len):\n",
    "        x_t = input[:, t, :].unsqueeze(2) # input[:,t,:].shape = [batch_size,input_size] -> (batch_size,input_size,1)\n",
    "\n",
    "        # w_ih_batch.shape = [hidden_size,input_size]->(1,hidden_size,input_size)->(batch_size,hidden_size,input_size)\n",
    "        # tile(batch_size, 1, 1): 第0维变成原来的batch_size倍（默认行复制）其他两维为1保持不动-> (batch_size,hidden_size,input_size)\n",
    "        w_ih_batch = W_ih.unsqueeze(0).tile(batch_size, 1, 1)\n",
    "\n",
    "        # w_hh_batch.shaoe = [hidden_size,input_size]->(1,hidden_size,input_size)->(batch_size,hidden_size,input_size)\n",
    "        w_hh_batch = W_hh.unsqueeze(0).tile(batch_size, 1, 1)\n",
    "\n",
    "        # w_ih_times_x.shape=(batch_size,hidden_size,1) -> (batch_size,hidden_size)\n",
    "        w_ih_times_x = torch.bmm(w_ih_batch, x_t).squeeze(-1)  # W_ih * x_t\n",
    "\n",
    "        # h_prev.unsqueeze(2) : (batch_size,hidden_size,1)\n",
    "        # w_hh_times_h.shape =(batch_size,hidden_size,1)->(batch_size,hidden_size)\n",
    "        w_hh_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        # h_prev = (1,batch_size,hidden_size)->(batch_size, hidden_size)\n",
    "        h_prev = torch.tanh(w_ih_times_x + b_ih + w_hh_times_h + b_hh)\n",
    "\n",
    "        h_output[:,t,:] = h_prev\n",
    "        \n",
    "    # 按官方api格式返回\n",
    "    # h_prev.unsqueeze(0) : (1,batch_size,hidden_size) 因为官方参数为(D∗num_layers,bs,hidden_size)\n",
    "    return h_output, h_prev.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70184ead-7f9a-48c7-9307-ac4cfedbf72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom\n",
      "tensor([[[ 0.8950,  0.4833, -0.5547],\n",
      "         [ 0.1424,  0.4373, -0.6090],\n",
      "         [-0.5399,  0.7522, -0.0882]],\n",
      "\n",
      "        [[ 0.2022,  0.8119, -0.3363],\n",
      "         [ 0.0075,  0.5236, -0.5589],\n",
      "         [ 0.6017,  0.2810, -0.6089]]], grad_fn=<TransposeBackward1>)\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[-0.5399,  0.7522, -0.0882],\n",
      "         [ 0.6017,  0.2810, -0.6089]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 2, 3])\n",
      "torch api\n",
      "tensor([[[ 0.8950,  0.4833, -0.5547],\n",
      "         [ 0.1424,  0.4373, -0.6090],\n",
      "         [-0.5399,  0.7522, -0.0882]],\n",
      "\n",
      "        [[ 0.2022,  0.8119, -0.3363],\n",
      "         [ 0.0075,  0.5236, -0.5589],\n",
      "         [ 0.6017,  0.2810, -0.6089]]], grad_fn=<CopySlices>)\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[-0.5399,  0.7522, -0.0882],\n",
      "         [ 0.6017,  0.2810, -0.6089]]], grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "rnn_output, h_n = rnn(input, h_prev.unsqueeze(0))\n",
    "custom_output, custom_hn = rnn_forward(input, rnn.weight_ih_l0, rnn.weight_hh_l0, rnn.bias_ih_l0, rnn.bias_hh_l0, h_prev)\n",
    "print('custom', rnn_output, rnn_output.shape, h_n, h_n.shape, sep='\\n')\n",
    "print('torch api', custom_output, custom_output.shape, custom_hn, custom_hn.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c5864-7dc1-4c9d-ac3f-91dbfa93aebb",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcb5b5-0f9c-4fe0-888b-f72f4209ee4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
