{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbc26a7-be2e-45d6-96c0-e50edd84e5f0",
   "metadata": {},
   "source": [
    "# RNN 简介\n",
    "\n",
    "# 时间序列模型概念简介\n",
    "循环神经网络（RNN）是一种神经网络类型，其神经元的输出在下一个时间步会反馈作为输入，使网络具有处理序列数据的能力。它能处理变长序列，挖掘数据中的时序信息，不过存在长期依赖问题，即难以处理长序列中相距较远的信息关联。\n",
    "RNN与普通神经网络的主要区别在于其具有记忆功能，神经元的输出能作为下一步输入，可处理序列数据，且输入和输出长度不固定；普通神经网络一般处理独立同分布的数据，层与层之间是简单的前馈连接关系，输入输出的长度通常是固定的。\n",
    "\n",
    "RNN的应用场景广泛，在自然语言处理方面，可用于语言模型来预测下一个单词的概率，还能完成机器翻译、文本生成任务；在语音识别领域，能够处理语音这种时间序列信号，提高识别准确率；在时间序列预测中，像股票价格预测、天气预测等，RNN通过学习历史数据模式预测未来值；在视频分析中，它可以处理视频帧序列，进行动作识别等操作。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RNN网络结构图\n",
    "![图1](images/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed32eb-d5dc-4fe6-9164-3d621a98d2fa",
   "metadata": {},
   "source": [
    "RNN公式：\n",
    "$$\n",
    "[\n",
    "\\boldsymbol{h}_t = tanh(\\boldsymbol{h}_{t-1} \\boldsymbol{W}_h + \\boldsymbol{x}_t \\boldsymbol{W}_x + \\boldsymbol{b})\n",
    "]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672e561-69a7-416d-93f3-39c6bb131577",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51511bf4-3182-4f3d-8482-5f36d3ab755b",
   "metadata": {},
   "source": [
    "# 观察torch.nn.RNN的输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e152dd0d-8abf-46d3-85d4-f418335ad2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "98eb8147-8728-49ac-b7ec-bbd4c85e27bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2211,  0.0713, -0.7325,  0.2592]]])\n",
      "tensor([[[-0.4568, -0.2468,  0.2100]]], grad_fn=<TransposeBackward1>)\n",
      "torch.Size([1, 1, 3])\n",
      "tensor([[[-0.4568, -0.2468,  0.2100]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 单向、单层rnn\n",
    "# 1个时间步\n",
    "# batch_first=True表示输入数据的维度为[batch_size, seq_len, input_dim], input_dim在后文也称为input_size\n",
    "single_rnn = nn.RNN(input_size=4, hidden_size=3, num_layers=1, batch_first=True) \n",
    "input = torch.randn(1, 1, 4) # 输入数据维度为[batch_size, time_steps_num, input_dim]\n",
    "output, h_n = single_rnn(input) # output维度为[batch_size, time_steps_num, hidden_size=3]，h_n维度为[num_layers=1, batch_size, hidden_size=3]\n",
    "print(input,output, output.shape, h_n, h_n.shape,  sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "92793422-5d08-49fd-bad4-7a18a428c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3055,  0.0048, -0.0629,  0.4583],\n",
      "         [ 0.8229,  1.5621,  0.1653,  0.5145]]])\n",
      "tensor([[[-0.2296, -0.4769, -0.0592],\n",
      "         [ 0.0134, -0.5053, -0.6914]]], grad_fn=<TransposeBackward1>)\n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[ 0.0134, -0.5053, -0.6914]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 单向、单层rnn\n",
    "# 2个时间步\n",
    "single_rnn = nn.RNN(input_size=4, hidden_size=3, num_layers=1, batch_first=True) # 输入数据的维度为[batch_size, time_steps_num, input_dim]\n",
    "input = torch.randn(1, 2, 4) # 输入数据维度为[batch_size, time_steps_num, input_dim]\n",
    "output, h_n = single_rnn(input) # output维度为[batch_size, time_steps_num, hidden_size=3]，h_n维度为[num_layers=1, batch_size, hidden_size=3]\n",
    "print(input,output, output.shape, h_n, h_n.shape,  sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40a4c3-8063-4b5c-8892-d0dce778f265",
   "metadata": {},
   "source": [
    "output输出为不同时间步的隐状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06c81d55-80f1-496e-8c00-57db4bf6edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0293, -0.2261,  0.3965, -0.8093,  0.5495, -0.2421],\n",
      "         [ 0.3172, -0.4707, -0.0548, -0.9589,  0.5515,  0.0759],\n",
      "         [-0.3819, -0.9026,  0.2700, -0.6062,  0.9286, -0.6791],\n",
      "         [-0.9650,  0.2898,  0.9175, -0.9964,  0.3749, -0.4732],\n",
      "         [ 0.4947, -0.6497,  0.0801, -0.3799,  0.8914, -0.4917]],\n",
      "\n",
      "        [[ 0.1236,  0.6172,  0.5129, -0.9334, -0.7831,  0.1077],\n",
      "         [ 0.7416,  0.5501,  0.4543, -0.8432, -0.2094, -0.3928],\n",
      "         [ 0.9069, -0.6283, -0.4312, -0.5202,  0.6983, -0.2993],\n",
      "         [ 0.2843, -0.9798, -0.5583, -0.0776,  0.9733,  0.1556],\n",
      "         [-0.9714, -0.1158,  0.7961, -0.9926,  0.1743,  0.1932]],\n",
      "\n",
      "        [[ 0.8565, -0.8896, -0.7905, -0.4024,  0.6848,  0.4695],\n",
      "         [-0.2559,  0.0835,  0.7091, -0.7468, -0.3244, -0.6832],\n",
      "         [-0.3923, -0.4974,  0.4001, -0.9646,  0.8942,  0.0540],\n",
      "         [ 0.2724, -0.8785, -0.4926, -0.8918,  0.8703,  0.0652],\n",
      "         [ 0.4889, -0.8752, -0.3374,  0.1035,  0.6077, -0.4534]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "torch.Size([3, 5, 6])\n",
      "tensor([[[ 0.4947, -0.6497,  0.0801],\n",
      "         [-0.9714, -0.1158,  0.7961],\n",
      "         [ 0.4889, -0.8752, -0.3374]],\n",
      "\n",
      "        [[-0.8093,  0.5495, -0.2421],\n",
      "         [-0.9334, -0.7831,  0.1077],\n",
      "         [-0.4024,  0.6848,  0.4695]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 双向、单层rnn\n",
    "bi_rnn = nn.RNN(input_size=4, hidden_size=3, num_layers=1, batch_first=True, bidirectional=True)\n",
    "bi_output, bi_h_n = bi_rnn(input)\n",
    "print(bi_output, bi_output.shape, bi_h_n, bi_h_n.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0b04f-d59f-421a-b5b8-6d8f87b8d06e",
   "metadata": {},
   "source": [
    "# 从零手搓 RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943a164-b997-4947-bce8-8618b421ef3d",
   "metadata": {},
   "source": [
    "### 自定义单向单层RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b52299c8-f30e-4f86-8f39-c1a35116dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46bc4c-6957-49af-bad3-42163c342945",
   "metadata": {},
   "source": [
    "对照RNN公式实现RNN Layer\n",
    "$$\n",
    "[\n",
    "\\boldsymbol{h}_t = tanh(\\boldsymbol{h}_{t-1} \\boldsymbol{W}_h + \\boldsymbol{x}_t \\boldsymbol{W}_x + \\boldsymbol{b})\n",
    "]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a73914e1-9e0f-4296-82f9-9397b2655353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers=1, batch_first=True):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.bidirectional = False\n",
    "        super().__init__()\n",
    "        self.W_ih = nn.Parameter(torch.rand(self.input_size, self.hidden_size))\n",
    "        self.W_hh = nn.Parameter(torch.rand(self.hidden_size, self.hidden_size))\n",
    "        self.b_ih = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        self.b_hh = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        \n",
    "    def forward(self,x_t,h_prev=None):\n",
    "        # part 1: torch.matmul(x_t, self.W_ih)\n",
    "        # x_t包含多个时间步，形状为[batch_size, time_steps_num, input_dim]\n",
    "        # W_ih形状为[input_dim, hidden_size]\n",
    "        # torch.matmul(x_t, self.W_ih) 输出矩阵形状为[batch_size, time_steps_num, hidden_size]\n",
    "        # part 2: torch.matmul(h_prev, self.W_hh)\n",
    "        # h_prev 形状为[batch_size, time_steps_num, hidden_size]\n",
    "        # W_hh形状为[hidden_size, hidden_size]\n",
    "        # torch.matmul(h_prev, self.W_hh) 输出矩阵形状为[batch_size, time_steps_num, hidden_size]\n",
    "        if h_prev == None:\n",
    "             h_prev = torch.zeros( x_t.size(0), self.hidden_size)\n",
    "        output = torch.tanh(torch.matmul(x_t, self.W_ih) + self.b_ih + torch.matmul(h_prev, self.W_hh) + self.b_hh)\n",
    "        return output,output[:,-1,:].unsqueeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9415e-d1c4-452d-8424-2f11f0d789f4",
   "metadata": {},
   "source": [
    "### 测试输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c66b3a31-7bee-4b8d-a72e-d21a8e2b886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9629,  0.9930,  0.7752],\n",
      "         [-0.2752, -0.1178, -0.2255],\n",
      "         [-0.4656, -0.5441, -0.2772],\n",
      "         [-0.7855, -0.8322,  0.0315],\n",
      "         [ 0.7842,  0.9167,  0.8217]]], grad_fn=<TanhBackward0>)\n",
      "torch.Size([1, 5, 3])\n",
      "tensor([[[0.7842, 0.9167, 0.8217]]], grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 单向、单层rnn\n",
    "single_rnn = RNNLayer(input_size=4, hidden_size=3, num_layers=1, batch_first=True) # batch_first=True表示输入数据的维度为[batch_size, time_steps_num, input_dim]\n",
    "input = torch.randn(1, 5, 4) # 输入数据维度为[batch_size, time_steps_num, input_size]\n",
    "output,h_n = single_rnn(input) # output维度为[batch_size, time_steps_num, hidden_size=3]，h_n维度为[num_layers=1, batch_size, hidden_size=3]\n",
    "print(output, output.shape, h_n, h_n.shape,  sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c2d2b-27bc-4e06-8c55-39cfdf95a6c9",
   "metadata": {},
   "source": [
    "输出结果形状与nn.RNN一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191a8f4-53f7-4c0b-b6c4-a1f95f54e0cd",
   "metadata": {},
   "source": [
    "### 用nn.RNN建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "30c62d15-3524-4d4b-b063-d65a183e680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ee0112ac-52fc-4d40-b946-2c2bcf193e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = output_size# 输入是One hot, output_size和vocab_size 都是词表大小\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 这里的X输入为word index\n",
    "        X = F.one_hot(torch.tensor(torch.tensor(X)),self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        print(X.size())\n",
    "        state_0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device) # 隐状态的形状为[层数，batch_size,hidden_size]\n",
    "        out, state = self.rnn(X, state_0) \n",
    "        out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a29b0-6c9f-479f-8db2-e30085795294",
   "metadata": {},
   "source": [
    "### 测试模型输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8d801-0fc9-47f3-ad9d-08adcd2c05bc",
   "metadata": {},
   "source": [
    "首先导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f8b165d6-da2f-4442-a5b1-83b211a69bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_lyrics():\n",
    "    #with zipfile.ZipFile('./test.txt') as zin:\n",
    "    with open('test.txt') as f:\n",
    "            corpus_chars = f.read()#.decode('utf-8')\n",
    "    # corpus_chars[:40]  # '想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'\n",
    "\n",
    "    # 将换行符替换成空格；仅使用前1万个字符来训练模型\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "\n",
    "    # 将每个字符映射成索引\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)  # 1027\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    sample = corpus_indices[:20]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "87ecffdd-81e8-4adb-a635-30b9aeb61a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f92e88-925b-41d0-b649-78718235c7d1",
   "metadata": {},
   "source": [
    "测试模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "50dc8609-91b2-4ef6-ab6b-1d896c042148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46949/1910301334.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = F.one_hot(torch.tensor(torch.tensor(X)),self.vocab_size)\n"
     ]
    }
   ],
   "source": [
    "model = CustomRNN(256, 10,1,256)\n",
    "Y = model([[13,12,14]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "598e5158-47e7-4ad0-8053-30fe70d058b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3c5520c5-56e7-490e-b765-95b9b6e75210",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Y.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6a2cfa8b-b426-4716-8801-3a616d6214af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'动'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2cb04-7fd9-45a3-9ad7-24d64a827cb3",
   "metadata": {},
   "source": [
    "把推理部分打包成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1b6ad78c-9daa-4cf0-8be8-e7178601b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(init_chars,model,time_steps_num,idx_to_char,char_to_idx):\n",
    "    X = []\n",
    "    for c in init_chars:\n",
    "        X.append(char_to_idx[c])\n",
    "    output = init_chars\n",
    "    print(X)\n",
    "    for i in range(time_steps_num):\n",
    "        Y= model([X])\n",
    "        idx = Y.argmax(dim=1)\n",
    "        X.append(idx)\n",
    "        output+=idx_to_char[idx]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2fce464f-6765-42f3-bb2f-8e629beb3486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 2, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 4, 256])\n",
      "torch.Size([1, 5, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46949/1910301334.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = F.one_hot(torch.tensor(torch.tensor(X)),self.vocab_size)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'构0G0G0'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('构', model,5,idx_to_char,char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48443935-8bee-417f-aec5-c1d66166f21c",
   "metadata": {},
   "source": [
    "模型尚未训练，输出随机结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17c9d9-8917-406c-a64c-ebaae0bbdb00",
   "metadata": {},
   "source": [
    "### 用自定义的RNN Layer预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d8a8528d-6394-40b5-b86d-3e92797a0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = output_size# 输入是One hot, output_size和vocab_size 都是词表大小\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = RNNLayer(input_size, hidden_size, num_layers, batch_first=True)  # 讲nn.RNN替换为自定义的RNNLayer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 这里的X输入为word index\n",
    "        X = F.one_hot(torch.tensor(torch.tensor(X)),self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        print(X.size())\n",
    "        state_0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device) # 隐状态的形状为[层数，batch_size,hidden_size]\n",
    "        out, state = self.rnn(X, state_0) \n",
    "        out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e543c60b-ed82-4060-b8ae-47d34f8b875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 2, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 4, 256])\n",
      "torch.Size([1, 5, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46949/1910301334.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = F.one_hot(torch.tensor(torch.tensor(X)),self.vocab_size)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'构形h端回h'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomRNN(256, 10,1,256)\n",
    "predict('构', model,5,idx_to_char,char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c6577-38b9-46fc-9911-6bb810d05a77",
   "metadata": {},
   "source": [
    "同样也是随机结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ca6fd-2982-4252-8bfe-e5abc3d4af1c",
   "metadata": {},
   "source": [
    "### 思考：\n",
    "- 如何构建多层、双向的RNN？\n",
    "- 如何使用自定义的RNN Layer及模型训练一个歌词或者诗词生成器？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d553b-4102-44f7-9ded-5df5686ec77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
